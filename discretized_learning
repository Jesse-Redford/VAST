#-------------------------------------------- Defining State-Action Space -----------------------------------------------------------#

state1 = range(0,4,1) # define the discritized state space
state2 = range(0,4,1) # range(lower_bound, upper_bound, step_size) 

action1 = range(0,4,1) # define the discritized action space 
action2 = range(0,4,1) # these will be the values sent to the servos so consider the physical constraints of the system and motors

states = list(itertools.product(state1,state2)) # Create a list of every state combination
actions = list(itertools.product(action1,action2)) # Create a list of every action combination

# ---------------------------------- Initalize Q_Table (zeros, random, pre-trained) --------------------------------------------------#

Q_zeros = pd.DataFrame(np.zeros((len(states), len(actions))), index=states, columns=actions, dtype=float)

Q_random = pd.DataFrame(np.random.uniform(0, 1, size=(len(states), len(actions))).round(4), index=states, columns=actions, dtype=float)

Qtable = Q_zeros 
    ---> run training session 
      ---> save trained Qtable 
        ---> Qtable.to_pickle('Trained_Qtable') 
          ---> Load file for new session
            ---> Qtable_trained = pd.read_pickle('Trained_Qtable')



# --------------------------------Accessing Qtable and required functions -------------------------------------#

def get_max_action(Qtable,state):
  action = Qtable.idxmax(axis=1).loc[[state]][0] # highested valued action
  return(action)
 
def random_action(actions):
    return(random.choice(actions))
    
def random_state(states):
    return random.choice(states) 

def Update_Qtable(Qtable,actions,state,action,new_state,alpha,gamma)
    Vnext = Qtable[actions].loc[[(new_state)]][:].max() # Max Q at new state
    Vmax=max(Vnext)
    Q=Qtable[action].loc[[(state)]][:].max()  # Past Qvalue of state-action pair
    Qtable[action].loc[[state]] = (1-alpha)*Q+alpha*(R+Vmax*gamma)
    return(Qtable)

def choose_action(Qtable,state,epsilion)
  if random.uniform(0, 1) > epsilion:
    action = Qtable.idxmax(axis=1).loc[[state]][0]
  else: 
    action = random_action()
  return(action)
  
def Beta(start,end):
    return math.exp(math.log(end/start)/maxsteps)
  


#----------------------------------------------- Transition Function -----------------------------------------------------------------#


#---------------------------------------------- Discritizing Sensory Inputs ----------------------------------------------------------#

def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return array[idx]


#----------------------------------------------------- Controller Functions --------------------------------------------------------#






#------------------------------------------------- Sensor Functions ----------------------------------------------------------------#











#--------------------------------------------- Qlearning _ Agent ------------------------------------------------------------------#







#----------------- Transition Functions ----------------------#

def transition(state,action)
  return(new_state,reward)



# ---------------------------------------Pre-training Simulator -----------------------------------------#

def Learning_Simulator(Qtable,states,actions,inital_state,eps_inital,eps_final,alpha,gamma,training_steps)

  state = initial_state
  step = 0
  
  while step < training_steps:
    epsilion = epsilion * Beta(epsi,epsf)
    action = choose_action(Qtable,state,epsilion)
    newstate, reward = transition(state,action)
    Qtable = Update_Qtable(Qtable,actions,state,action,new_state,alpha,gamma)
    state = newstate
    step = step + 1
    
    
    
    
    
  
  









